---
title: "R for Data Cleaning and Merging"
source: Rmd
teaching: 75
exercises: 15
questions:
- "How can I combine two datasets from different sources?"
- "How can data tidying facilitate answering analysis questions?"
objectives:
- "To become familiar with more functions of the `dplyr` and `tidyr` packages."
- "To be able to use `dplyr` and `tidyr` to prepare data for analysis."
- "To be able to combine two different data sources using joins."
keypoints:
- "Assessing data source and structure is an important first step in analysis."
- "Data analysis in R facilitates reproducible research."
- "Preparing data for analysis can take significant effort and planning."
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("04-")
library(tidyverse)
```

### Contents

1.  [Cleaning up data](#cleaning-up-data)
1.  [Joining data frames](#joining-data-frames)
1.  [Analyzing combined data](#analyzing-combined-data)

# Cleaning up data

[*Back to top*](#contents)

Researchers are often pulling data from several sources, and the process of making data compatible with one another and prepared for analysis can be a large undertaking. Luckily, there are many functions that allow us to do this in R. We've been working with the Global Burden of Disease (GBD 2019) dataset, which contains population, smoking rates, and lung cancer rates by year. In this section, we practice cleaning and preparing a second dataset containing ambient pollution data by location and year, also sourced from [the GBD 2019](https://ghdx.healthdata.org/record/global-burden-disease-study-2019-gbd-2019-air-pollution-exposure-estimates-1990-2019).

It's always good to go into data cleaning with a clear goal in mind. Here, we'd like to prepare the ambient pollution data to be compatible with our lung cancer data so we can directly compare lung cancer rates to ambient pollution levels. To make this work, we'd like a data frame that contains columns with the country name, year, and median ambient pollution levels (in micrograms per cubic meter). We will make this comparison for the most recent year in these datasets, 2019. Let's start with reading the data in using `read_csv()`.

```{r CleanEmissionsData}
read_csv("data/ambient_pollution.csv")
```

It looks like our data object has three columns: `location_name`, `year_id`, and `median`. Scroll through the data object to get an idea of what's there. Can you think of anything we might need to take care of in order to merge these data with our lung cancer rates dataset? It looks like the `location_name` column contains values other than countries, and our `year_id` column has many years, and we are only interested in 2019 for now.

Let's save this dataframe to `ambient_pollution_dirty` so that we don't have to read it in every time we want to clean it.

```{r ambient_pollution_dirty}
ambient_pollution_dirty <- read_csv("data/ambient_pollution.csv")
ambient_pollution_dirty
```

Alright, before we start changing anything about our dataset, it can be helpful to get a better idea of what we are looking at. For example, it's helpful to use `count()` to get an idea of how many observations we have for particular values in a column. Let's start by looking at how many entries we have for each year in the dataset. This will also help us determine whether we have data for the year 2019, which is what we are looking for.

```{r countyear}
ambient_pollution_dirty %>%
  group_by(year_id) %>%
  count()
```

This output tells us two useful things: (1) for which years we have data, and (2) how many observations we have for each year. The summary data has 14 rows, which means there are 14 different years in which the pollution data was collected between 1990 and 2019. If you scroll to the end, you can see that we have 690 entries for 2019, which is excellent, since that's the year we are most interested in. It also looks like the number of observations is consistently 690 for each year that we have pollution measurements. 

We previously saw how we can subset rows from a data frame using `filter()`. Let's subset out the rows we are interested in, namely those from 2019. We can also drop the `year_id` column, since we know these data are only from 2019. Let's store the resulting dataset in an object named `pollution_2019_dirty`.

```{r filter2019}
pollution_2019_dirty <- ambient_pollution_dirty %>%
  filter(year_id == 2019) %>%
  select(-year_id)
```

Notice that this data object has 690 rows, as we expect from our previous data summary.

Sometimes it can be helpful to explore your data summaries in the view tab. Try it in the exercise below.

> ## Viewing data summaries
> Count the number of observations for each unique `location_name` in `pollution_2019_dirty` and use the *pipe operator* and `view()` to explore the summary data. Click on the column names to reorder the summary data however you'd like. Do you notice anything interesting?
> 
> > ## Solution: 
> > ```{r count_location_name}
> > pollution_2019_dirty %>%
> >   group_by(location_name) %>%
> >   count() %>%
> >   view()
> > ```
> > There are a couple things to notice. First, the summary data only has 685 rows, even though there were 690 observations for each year. This suggests that there may be multiple measurements for some location_name values. Second, if you click on the `n` column to sort in descending order, you can see that there are five location_name values for which we have 2 observations instead of 1. This is good to know as we continue to learn more about our datset.
> {: .solution}
{: .challenge}


Another helpful thing to do when exploring and cleaning a new dataset is to get an idea of whether there are any missing values or duplicate rows. Let's start by checking to see if we have any **missing data**. We will do this using `drop_na()`, a tidyverse function that removes any rows that have missing values. We will then check the number of rows using `nrow` in our dataset and compare to the original to see if we lost any rows with missing data.

```{r drop_na}
pollution_2019_dirty %>%
  drop_na() %>%
  nrow()
```

After dropping rows with missing values, our data object still has the same number of rows (690) as `pollution_2019_dirty`. This tells us that we don't have any rows with missing data. Wow! We're pretty lucky, because often datasets DO have missing data. What if we wanted to identify which observations had some missing values? Try the exercise below to find out!

> ## Filtering for missing data
> Filter `pollution_2019_dirty` only to rows which have missing values in the `location_name` column. Repeat for `year_id` and `median`. *Hint: `is.na()` is a helpful function for identifying missing data!* 
> 
> > ## Solution: 
> > ```{r isna}
> > 
> > pollution_2019_dirty %>%
> >   filter(is.na(location_name))
> > 
> > pollution_2019_dirty %>%
> >   filter(is.na(year_id))
> > 
> > pollution_2019_dirty %>%
> >   filter(is.na(median))
> > ```
> > 
> > You can see that all three pieces of code return empty data objects. This is good and what we expect, since we didn't find rows with missing values using `drop_na()` above.
> {: .solution}
{: .challenge}


Now, let's check to see if our dataset contains *duplicate rows*. We already know that we have some rows with identical location names, but are the rows identical? We can use the `distinct()` function, which removes any rows for which all values are duplicates of another row, followed by `nrow()` to find out:
```{r distinct}

pollution_2019_dirty %>%
  distinct() %>%
  nrow()

```
You can see that after applying the `distinct()` function, our dataset only has 688 rows. This tells us that there are two rows that were exactly identical to other rows in our dataset.

> ## Exercise: Filtering for duplicate data
> Filter `pollution_2019_dirty` only to rows which have duplicate values in the `location_name` column. *Hint: try using `group_by()` and `mutate()` to count the number of rows for each `location_name`.* 
> 
> > ## Solution: 
> > ```{r isna}
> > 
> > pollution_2019_dirty %>%
> >   group_by(location_name) %>%
> >   mutate( n = n()) %>%
> >   filter(n > 1) %>%
> >   arrange(location_name)
> > ```
> > 
> > You can see that we get a data object with 10 entries, 2x5 location_names. Two of these location_names have identical rows, while the others have two different values, which presumably could have been measured at different times in 2019.
> {: .solution}
{: .challenge}

Let's go ahead and take care of the location_names which have two different median pollution values by making a new column called `pollution` that is the mean. We can then remove the `median` column and store the resulting data object as `pollution_2019`.
```{r pollution2019}

pollution_2019 <- pollution_2019_dirty %>%
  group_by(location_name) %>%
  mutate(pollution = mean(median)) %>%
  select(-median) %>%
  distinct()

```

You can see that `pollution_2019` has 685 rows, as we expect, since we took care of the duplicated location_names.

Note: here, we took the mean to take care of duplicates and multiple entries, but this is not always the best way to do so. When working with your own data, make sure to think carefully about your dataset, what these multiple entries really mean, and whether you want to leave them as they are or take care of them in some different way.

> **Looking at your data:** You can get a look at your data-cleaning hard work by navigating to the **Environment** tab in RStudio and clicking the table icon next to the variable name. Notice when we do this, RStudio automatically runs the `View()` command. We've made a lot of progress!

{.callout}

# Joining data frames

[*Back to top*](#contents)


Now we're almost ready to join our pollution data to the smoking and lung cancer data. Previously we saw that we could read in and filter the smoking and lung cancer data to get the data from 2019 to create a new dataframe with our filtered data:

```{r LoadGapminder2007}
smoking_2019 <- read_csv("data/smoking_cancer.csv") %>%
  filter(year == 2019) %>%
  select(-year)
```

Look at the data in `pollution_2019` and `smoking_2019`. If you had to merge these two data frames together, which columns would you use to merge them together? If you said `location_name` and `country`, you're right! But before we join the datasets, we need to make sure these columns are named the same thing.

> ## Review: Re-naming columns
> Rename the `location_name` column to `country` in the `pollution_2019` dataset. Go ahead and overwrite and store in the `pollution_2019` object
> 
> > ## Solution: 
> > ```{r isna}
> > 
> > pollution_2019 <- pollution_2019 %>%
> >   rename(country = location_name) 
> > ```
> > 
> > Note that the column is labeled `country` even though it has values beyond the names of countries. We will take care of this later when we join datasets.
> {: .solution}
{: .challenge}

Because the column is now present in both datasets, we'll call country our "key". Now, when we join them together, can you think of any problems we might run into when we merge things? We might not have pollution data for all of the countries in the `smoking_2019` dataset and vice versa. Also, a country might be represented in both data frames but not by the same name in both places.

The dplyr package has a number of tools for joining data frames together depending on what we want to do with the rows of the data of countries that are not represented in both data frames. Here we'll be using `inner_join()` and `anti_join()`. 

In an "inner join", the new data frame only has those rows where the same key is found in both data frames. This is a very commonly used join.

![]({{ page.root }}/fig/r-data-analysis/join-inner.png)

> ## Bonus: Other dplyr join functions 
>
> Outer joins and can be performed using `left_join()`, `right_join()`, and `full_join()`. In a "left join", if the key is present in the left hand data frame, it will appear in the output, even if it is not found in the the right hand data frame. For a right join, the opposite is true. For a full join, all possible keys are included in the output data frame.
> 
> ![]({{ page.root }}/fig/r-data-analysis/join-outer.png)
{: .solution}



Let's give the `inner_join()` function a try.
```{r InnerJoin}
inner_join(smoking_2019, pollution_2019)
```

Do you see that we now have data from both data frames joined together in the same data frame? One thing to note about the output is that `inner_join()` tells us that that it joined by "country". We can make this explicit using the "by" argument in the join functions

```{r InnerJoinByCountry}
inner_join(smoking_2019, pollution_2019, by="country")
```

One thing to notice is that `smoking_2019` has 191 rows, but the output of our join only has 189 rows. Let's investigate. It appears that there must have been countries in the `smoking_2019` data that did not appear in our `pollution_2019` data object. 

Let's use `anti_join()` for this - this will show us the data for the keys on the left that are missing from the data frame on the right. 

```{r AntiJoin}
anti_join(smoking_2019, pollution_2019, by="country")
```
We can see that the pollution_2019 data were missing for Vietnam and Slovak Republic. 

If we look at the pollution_2019 data with `View()` and sort by `country`, we can see that Vientam and Slovak Republic are called different things in the pollution_2019 dataframe. They're called "Viet Nam" and "Slovakia," respectively. Using `mutate()` and `recode()`, we can update the pollution_2019 data so that the country names for Vietnam and Slovak Republic match those in the smoking_2019 data.

```{r pollutionRecode}

pollution_2019 <- pollution_2019 %>%
  mutate(country = recode(country, "Viet Nam" = "Vietnam", "Slovakia" = "Slovak Republic"))


anti_join(smoking_2019, pollution_2019, by="country")
```

Now our `anti_join()` returns an empty data frame, which tells us that we have reconciled all of the keys from the smoking_2019 data with the data in the pollution_2019 data frame.

Finally, let's use the `inner_join()` to create a new data frame:

```{r CleanInnerJoin}
smoking_pollution <- inner_join(smoking_2019, pollution_2019, by="country")
```



We have reached our data cleaning goal! One of the best aspects of doing all of these steps coded in R is that our efforts are reproducible, and the raw data is maintained. With good documentation of data cleaning and analysis steps, we could easily share our work with another researcher who would be able to repeat what we've done. However, it's also nice to have a saved `csv` copy of our clean data. That way we can access it later without needing to redo our data cleaning, and we can also share the cleaned data with collaborators. To save our dataframe, we'll use `write_csv()`. 

```{r writeCSV}
write_csv(smoking_pollution, "data/smoking_pollution.csv")
```

Great - Now we can move on to the analysis! 

# Analyzing combined data

[*Back to top*](#contents)

For our analysis, we have three questions we'd like to answer: (1) Is there a relationship between population and ambient pollution levels (in micrograms per cubic meter)?; (2) which continent has the highest pollution levels per capita?; and (3) Is there a relationship between ambient pollution levels per capita and lung cancer rates?

**(1) Is there a relationship between ambient pollution levels (in micrograms per cubic meter)?**

To answer this question, we'll plot ambient pollution levels against population using a scatter plot. It will help to scale the x axis (population) log 10.

```{r PlotPercapCO2vsGDP}
smoking_pollution %>%
  ggplot() +
  aes(x = pop, y = pollution) +
  geom_point() +
  scale_x_log10() +
  labs(x = "Population", y = "Ambient pollution levels (micrograms/cubic meter)", size = "Population\n(millions)") +
  theme_bw()
  
```

We observe a positive association between ambient pollution levels and population.

To help clarify the association, we can add a fit line through the data using `geom_smooth(method = "lm")`. Notice we added the `method = "lm"` argument. This tells `geom_smooth()` that we would like a linear model (lm) fit to the data.

```{r PlotPercapCO2vsGDPSmooth}
smoking_pollution %>%
  ggplot() +
  aes(x = pop, y = pollution) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10() +
  labs(x = "Population", y = "Ambient pollution levels (micrograms/cubic meter)", size = "Population\n(millions)") +
  theme_bw()

```


To answer our first question, we observe a positive association between population and ambient pollution. In other words, countries with higher populations tend to have higher ambient pollution levels. It is very important to remember that associations are not indicative of causality and there could be confounding variables that may be playing into this apparent relationship. Can you think of any confounding factors we haven't accoutned for?

**2) Which continent has the highest pollution levels per capita?**

To answer this question, we need to calculate the pollution levels per capita for each country using `mutate()`. Then plot a boxplot to look at these levels by continent. *Hint: it may help to scale the y axis log10*

```{r pollutionPerCaptia}
smoking_pollution %>%
  mutate(pollution_capita = pollution/pop) %>%
  ggplot() +
  aes(x = continent, y = pollution_capita) +
  geom_boxplot() +
  scale_y_log10() +
  labs(y = "Pollution (micrograms/cubic meter) per capita")+
  theme_bw() 
```

Which continent has the highest pollution levels per capita? What other factors do you think could be driving this observation?

**3) Is there a relationship between ambient pollution levels per capita and lung cancer rates?**

To answer this question, let's make a scatter plot with ambient pollution levels on the x axis and lung cancer rates on the y axis. Make sure to scale the x-axis log10.
```{r}
smoking_pollution %>%
  mutate(pollution_capita = pollution/pop) %>%
  ggplot() +
  aes(x = pollution_capita, y = lung_cancer_pct) +
  geom_point() +
  scale_x_log10() +
  labs(x = "Pollution (micrograms/cubic meter) per capita", y = "Percent of people with lung cancer") +
  theme_bw()
```

There does not appear to be a direct relationship between pollution and lung cancer rates. 


# Bonus 

## Bonus exercise

> ## Calculating percent
>
> What percentage of the population in Africa does Kenya make up? What percentage of the global population does Africa make up? 
>
> > ## Solution
> >
> > Create a new variable using `group_by()` and `mutate()` that calculates percentages for the pop variable.
> >
> > ```{r PopPercExercise}
> > smoking_pollution %>%
> >   mutate(total_pop = sum(pop)) %>%
> >   group_by(continent) %>% 
> >   mutate(cont_pop = sum(pop), cont_percent = cont_pop/total_pop * 100, country_cont_pct = pop/cont_pop * 100) %>%
> >   select(country, continent, cont_percent, country_cont_pct) %>%
> >   filter(country == "Kenya")
> > ```
> >
> > This table shows that Kenya makes up 4% of the population of Africa, and Africa makes up 17% of the global population. 
> >

> > 
> {: .solution}
{: .challenge}

